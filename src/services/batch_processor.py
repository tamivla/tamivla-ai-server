# src/services/batch_processor.py
from typing import List, Tuple
import torch
import time
from loguru import logger

class VolumeBatchProcessor:
    """
    Volume-based –±–∞—Ç—á–µ—Ä –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    –§–æ—Ä–º–∏—Ä—É–µ—Ç –±–∞—Ç—á–∏ based –Ω–∞ –æ–±—ä–µ–º–µ –ø–∞–º—è—Ç–∏, –∞ –Ω–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ —Ç–µ–∫—Å—Ç–æ–≤
    """
    
    def __init__(self):
        self.memory_per_char = 0.3  # –í—Ä–µ–º–µ–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
        self.is_calibrated = False
        logger.info("üîß Batch processor –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω, –∫–∞–ª–∏–±—Ä–æ–≤–∫–∞ –æ—Ç–ª–æ–∂–µ–Ω–∞")
    
    def _calibrate_memory_usage(self) -> float:
        """
        –ê–í–¢–û–ö–ê–õ–ò–ë–†–û–í–ö–ê: —Ç–æ—á–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è –ø–∞–º—è—Ç–∏ –Ω–∞ —Å–∏–º–≤–æ–ª
        –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø—Ä–∏ –ü–ï–†–í–û–ú –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –±–∞—Ç—á–µ—Ä–∞
        """
        try:
            from services.model_manager import model_manager
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞
            model_name = "models--intfloat--multilingual-e5-large-instruct"
            if model_name not in model_manager.loaded_models:
                logger.warning("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –¥–ª—è –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–Ω—Å—Ç–∞–Ω—Ç—É")
                return 0.3  # Fallback
            
            model = model_manager.loaded_models[model_name]['model']
            
            # –¢–µ—Å—Ç–æ–≤—ã–µ —Ç–µ–∫—Å—Ç—ã —Ä–∞–∑–Ω–æ–π –¥–ª–∏–Ω—ã
            test_texts = [
                "A" * 100,    # –ö–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç
                "A" * 500,    # –°—Ä–µ–¥–Ω–∏–π —Ç–µ–∫—Å—Ç  
                "A" * 1000,   # –î–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
                "A" * 2000    # –û—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
            ]
            
            # –ó–∞–º–µ—Ä—è–µ–º –ø–∞–º—è—Ç—å –î–û –æ–±—Ä–∞–±–æ—Ç–∫–∏
            torch.cuda.empty_cache()
            initial_memory = torch.cuda.memory_allocated()
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ —Ç–µ–∫—Å—Ç—ã
            start_time = time.time()
            embeddings = model.encode(test_texts)
            processing_time = time.time() - start_time
            
            # –ó–∞–º–µ—Ä—è–µ–º –ø–∞–º—è—Ç—å –ü–û–°–õ–ï –æ–±—Ä–∞–±–æ—Ç–∫–∏
            final_memory = torch.cuda.memory_allocated()
            memory_used = final_memory - initial_memory
            
            # –í—ã—á–∏—Å–ª—è–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤
            total_chars = sum(len(text) for text in test_texts)
            
            # –í—ã—á–∏—Å–ª—è–µ–º –ø–∞–º—è—Ç—å –Ω–∞ —Å–∏–º–≤–æ–ª
            memory_per_char = memory_used / total_chars if total_chars > 0 else 0.3
            
            logger.info(f"üéØ –ö–∞–ª–∏–±—Ä–æ–≤–∫–∞: {len(test_texts)} —Ç–µ–∫—Å—Ç–æ–≤, {total_chars} —Å–∏–º–≤–æ–ª–æ–≤")
            logger.info(f"üéØ –ü–∞–º—è—Ç—å: {memory_used/1024**2:.2f} MB, –í—Ä–µ–º—è: {processing_time:.3f}s")
            logger.info(f"üéØ –†–µ–∑—É–ª—å—Ç–∞—Ç: {memory_per_char:.4f} –±–∞–π—Ç/—Å–∏–º–≤–æ–ª")
            
            # –û—á–∏—â–∞–µ–º –ø–∞–º—è—Ç—å
            del embeddings
            torch.cuda.empty_cache()
            
            return max(0.1, min(1.0, memory_per_char))  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑—É–º–Ω—ã–µ –ø—Ä–µ–¥–µ–ª—ã
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏: {e}, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–Ω—Å—Ç–∞–Ω—Ç—É 0.3")
            return 0.3  # Fallback –∑–Ω–∞—á–µ–Ω–∏–µ
    
    def _ensure_calibrated(self):
        """–£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞"""
        if not self.is_calibrated:
            self.memory_per_char = self._calibrate_memory_usage()
            self.is_calibrated = True
            logger.info(f"üîß Auto-calibrated memory per char: {self.memory_per_char:.4f} bytes")
    
    def estimate_text_volume(self, text: str) -> int:
        """
        –ë—ã—Å—Ç—Ä–∞—è –æ—Ü–µ–Ω–∫–∞ –æ–±—ä–µ–º–∞ –ø–∞–º—è—Ç–∏ –¥–ª—è —Ç–µ–∫—Å—Ç–∞
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —É—Å–ª–æ–≤–Ω—ã–µ –µ–¥–∏–Ω–∏—Ü—ã –æ–±—ä–µ–º–∞
        """
        return max(1, len(text))
    
    def calculate_max_volume(self) -> int:
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –æ–±—ä–µ–º –±–∞—Ç—á–∞ based –Ω–∞ —Å–≤–æ–±–æ–¥–Ω–æ–π –ø–∞–º—è—Ç–∏ GPU
        """
        # üî¥ –í–´–ü–û–õ–ù–Ø–ï–ú –ö–ê–õ–ò–ë–†–û–í–ö–£ –ü–†–ò –ü–ï–†–í–û–ú –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ò
        self._ensure_calibrated()
        
        if not torch.cuda.is_available():
            return 10000  # Fallback –¥–ª—è CPU
            
        # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–∞–º—è—Ç–∏
        allocated = torch.cuda.memory_allocated()
        total = torch.cuda.get_device_properties(0).total_memory
        free_memory = total - allocated
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º 70% —Å–≤–æ–±–æ–¥–Ω–æ–π –ø–∞–º—è—Ç–∏ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
        safe_memory = int(free_memory * 0.7)
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –±–∞–π—Ç—ã –≤ —É—Å–ª–æ–≤–Ω—ã–µ –µ–¥–∏–Ω–∏—Ü—ã –æ–±—ä–µ–º–∞
        max_volume = int(safe_memory / self.memory_per_char)
        
        logger.debug(f"üéØ Free: {free_memory/1024**2:.0f}MB -> Max volume: {max_volume}")
        
        return max(1000, max_volume)  # –ú–∏–Ω–∏–º—É–º 1000 –µ–¥–∏–Ω–∏—Ü
    
    def form_batches(self, texts: List[str]) -> List[List[str]]:
        """
        –§–æ—Ä–º–∏—Ä—É–µ—Ç –±–∞—Ç—á–∏ based –Ω–∞ –æ–±—ä–µ–º–µ –ø–∞–º—è—Ç–∏
        """
        if not texts:
            return []
            
        max_volume = self.calculate_max_volume()
        batches = []
        current_batch = []
        current_volume = 0
        
        for text in texts:
            text_volume = self.estimate_text_volume(text)
            
            # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –æ–¥–∏–Ω —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω–æ
            if text_volume > max_volume:
                logger.warning(f"üìè –¢–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π: {text_volume} > {max_volume}")
                if current_batch:
                    batches.append(current_batch)
                    current_batch = []
                    current_volume = 0
                batches.append([text])
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–ª–µ–∑–∞–µ—Ç –ª–∏ —Ç–µ–∫—Å—Ç –≤ —Ç–µ–∫—É—â–∏–π –±–∞—Ç—á
            if current_volume + text_volume > max_volume and current_batch:
                # –ë–∞—Ç—á –∑–∞–ø–æ–ª–Ω–µ–Ω - —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∏ –Ω–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π
                batches.append(current_batch)
                current_batch = [text]
                current_volume = text_volume
            else:
                # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Ç–µ–∫—É—â–∏–π –±–∞—Ç—á
                current_batch.append(text)
                current_volume += text_volume
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –±–∞—Ç—á –µ—Å–ª–∏ –æ–Ω –Ω–µ –ø—É—Å—Ç–æ–π
        if current_batch:
            batches.append(current_batch)
            
        logger.info(f"üì¶ –°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–æ –±–∞—Ç—á–µ–π: {len(batches)} –¥–ª—è {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤")
        
        return batches

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
batch_processor = VolumeBatchProcessor()
# src/services/embedding_service.py
from typing import List, Dict, Any, Optional
import numpy as np
from loguru import logger
from sentence_transformers import SentenceTransformer

from services.model_manager import model_manager
from services.batch_processor import batch_processor

class EmbeddingService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ç–µ–∫—Å—Ç–æ–≤—ã–º–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏"""
    
    def __init__(self):
        self.default_model = "models--intfloat--multilingual-e5-large-instruct"
        
    async def get_embeddings(self, texts: List[str], model_name: Optional[str] = None) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤
        """
        try:
            if not texts:
                return {
                    "object": "list",
                    "data": [],
                    "model": model_name or self.default_model,
                    "usage": {"prompt_tokens": 0, "total_tokens": 0}
                }
                
            model_to_use = model_name or self.default_model
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞
            if model_to_use not in model_manager.loaded_models:
                logger.info(f"Loading model: {model_to_use}")
                success = model_manager.load_model(model_to_use, "embedding")
                if not success:
                    return {
                        "object": "list",
                        "data": [],
                        "model": model_to_use,
                        "error": f"Failed to load model: {model_to_use}"
                    }
            
            # –ü–æ–ª—É—á–∞–µ–º –º–æ–¥–µ–ª—å
            model = model_manager.loaded_models[model_to_use]['model']
            
            # üî• –ò–°–ü–û–õ–¨–ó–£–ï–ú VOLUME-BASED –ë–ê–¢–ß–ò–ù–ì
            logger.info(f"üîÑ –ù–∞—á–∞–ª–æ volume-based –±–∞—Ç—á–∏–Ω–≥–∞ –¥–ª—è {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤")
            batches = batch_processor.form_batches(texts)
            logger.info(f"üì¶ –°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–æ –±–∞—Ç—á–µ–π: {len(batches)}")
            
            all_embeddings = []
            
            for i, batch in enumerate(batches):
                if batch:
                    logger.info(f"üî® –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ {i+1}/{len(batches)} —Ä–∞–∑–º–µ—Ä–æ–º {len(batch)} —Ç–µ–∫—Å—Ç–æ–≤")
                    batch_embeddings = model.encode(batch).tolist()
                    all_embeddings.extend(batch_embeddings)
            
            logger.info(f"‚úÖ Volume-based –±–∞—Ç—á–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω")
            
            # üî¥ OPENAI-–°–û–í–ú–ï–°–¢–ò–ú–´–ô –§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê
            response_data = []
            for i, embedding in enumerate(all_embeddings):
                response_data.append({
                    "object": "embedding",
                    "embedding": embedding,
                    "index": i
                })
            
            total_tokens = sum(len(text) for text in texts)
            
            return {
                "object": "list",
                "data": response_data,
                "model": model_to_use,
                "usage": {
                    "prompt_tokens": total_tokens,
                    "total_tokens": total_tokens
                },
                "batches_used": len(batches)  # üî• –ù–ê–®–ï –ö–ê–°–¢–û–ú–ù–û–ï –ü–û–õ–ï
            }
            
        except Exception as e:
            logger.error(f"Embedding error: {e}")
            return {
                "object": "list",
                "data": [],
                "model": model_name or self.default_model,
                "error": f"Embedding error: {str(e)}"
            }

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
embedding_service = EmbeddingService()
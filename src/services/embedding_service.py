# src/services/embedding_service.py
from typing import List, Dict, Any, Optional
import numpy as np
from loguru import logger
from sentence_transformers import SentenceTransformer

from services.model_manager import model_manager
from services.batch_processor import batch_processor

class EmbeddingService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ç–µ–∫—Å—Ç–æ–≤—ã–º–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏"""
    
    def __init__(self):
        self.default_model = "intfloat/multilingual-e5-large-instruct"  # ‚Üê HF –°–¢–ê–ù–î–ê–†–¢
        
    async def get_embeddings(self, texts: List[str], model_name: Optional[str] = None) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤
        """
        try:
            if not texts:
                return {
                    "object": "list",
                    "data": [],
                    "model": model_name or self.default_model,
                    "usage": {"prompt_tokens": 0, "total_tokens": 0}
                }
                
            model_to_use = model_name or self.default_model
            
            # üî¥ –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê: –∏—Å–ø–æ–ª—å–∑—É–µ–º model_manager.is_model_loaded()
            if not model_manager.is_model_loaded(model_to_use):
                logger.info(f"Loading model: {model_to_use}")
                success = model_manager.load_model(model_to_use, "embedding")
                if not success:
                    return {
                        "object": "list",
                        "data": [],
                        "model": model_to_use,
                        "error": f"Failed to load model: {model_to_use}"
                    }
            
            # –ü–æ–ª—É—á–∞–µ–º –º–æ–¥–µ–ª—å
            model = model_manager.get_model(model_to_use)
            
            # üî• –ò–°–ü–û–õ–¨–ó–£–ï–ú VOLUME-BASED –ë–ê–¢–ß–ò–ù–ì
            batches = batch_processor.form_batches(texts)
            all_embeddings = []
            
            for batch in batches:
                if batch:
                    batch_embeddings = model.encode(batch).tolist()
                    all_embeddings.extend(batch_embeddings)
            
            # üî¥ OPENAI-–°–û–í–ú–ï–°–¢–ò–ú–´–ô –§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê
            response_data = []
            for i, embedding in enumerate(all_embeddings):
                response_data.append({
                    "object": "embedding",
                    "embedding": embedding,
                    "index": i
                })
            
            total_tokens = sum(len(text) for text in texts)
            
            return {
                "object": "list",
                "data": response_data,
                "model": model_to_use,
                "usage": {
                    "prompt_tokens": total_tokens,
                    "total_tokens": total_tokens
                },
                "batches_used": len(batches)  # üî• –ù–ê–®–ï –ö–ê–°–¢–û–ú–ù–û–ï –ü–û–õ–ï
            }
            
        except Exception as e:
            logger.error(f"Embedding error: {e}")
            return {
                "object": "list",
                "data": [],
                "model": model_name or self.default_model,
                "error": f"Embedding error: {str(e)}"
            }

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
embedding_service = EmbeddingService()
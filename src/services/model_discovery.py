# src\services\model_discovery.py
"""
–°–µ—Ä–≤–∏—Å –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∏ –∞–Ω–∞–ª–∏–∑–∞ –º–æ–¥–µ–ª–µ–π –≤ –∫–µ—à–µ
–ü–æ–¥–¥–µ—Ä–∂–∫–∞ GGUF —Ñ–æ—Ä–º–∞—Ç–∞ –¥–ª—è LLM –∏ HF —Ñ–æ—Ä–º–∞—Ç–∞ –¥–ª—è embeddings
"""

import os
import json
from pathlib import Path
from typing import Dict, List, Any, Optional
from loguru import logger
from huggingface_hub import snapshot_download

class ModelDiscoveryService:
    """–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∏ –∞–Ω–∞–ª–∏–∑ –º–æ–¥–µ–ª–µ–π –≤ –ª–æ–∫–∞–ª—å–Ω–æ–º –∫–µ—à–µ"""
    
    def __init__(self):
        self.models_cache = Path(os.environ.get('HF_HOME', 'storage/models'))
        
    def scan_models_cache(self) -> Dict[str, Any]:
        """
        –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞–ø–∫–∏ —Å –º–æ–¥–µ–ª—è–º–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π GGUF –∏ HF —Ñ–æ—Ä–º–∞—Ç–æ–≤
        """
        try:
            logger.info(f"üîç –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–µ—à–∞ –º–æ–¥–µ–ª–µ–π: {self.models_cache}")
            
            if not self.models_cache.exists():
                return {"error": "–ü–∞–ø–∫–∞ —Å –º–æ–¥–µ–ª—è–º–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç", "models": []}
            
            models_info = []
            
            # –°–∫–∞–Ω–∏—Ä—É–µ–º –í–°–ï —ç–ª–µ–º–µ–Ω—Ç—ã –≤ –∫–µ—à–µ
            for item in self.models_cache.iterdir():
                if item.is_file() and item.suffix.lower() == '.gguf':
                    # –ù–∞–π–¥–µ–Ω GGUF —Ñ–∞–π–ª - –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∫ LLM –º–æ–¥–µ–ª—å
                    model_info = self._analyze_gguf_file(item)
                    if model_info and self._is_usable_model(model_info):
                        models_info.append(model_info)
                        logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω GGUF: {item.name}")
                        
                elif item.is_dir() and self._is_valid_model_directory(item):
                    # –ù–∞–π–¥–µ–Ω–∞ –ø–∞–ø–∫–∞ –≤ HF —Ñ–æ—Ä–º–∞—Ç–µ
                    model_info = self.analyze_model_directory(item)
                    if model_info and self._is_usable_model(model_info):
                        models_info.append(model_info)
                        logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω HF: {item.name} ({model_info.get('type', 'unknown')})")
            
            # –°–û–†–¢–ò–†–£–ï–ú –º–æ–¥–µ–ª–∏ –ø–æ —Ç–∏–ø—É –∏ –∫–∞—á–µ—Å—Ç–≤—É
            models_info.sort(key=lambda x: (
                0 if x.get('type') == 'embedding' else 1,  # –°–Ω–∞—á–∞–ª–∞ embedding –º–æ–¥–µ–ª–∏
                x['name']  # –ó–∞—Ç–µ–º –ø–æ –∏–º–µ–Ω–∏
            ))
            
            result = {
                "cache_path": str(self.models_cache),
                "total_models": len(models_info),
                "models": models_info
            }
            
            logger.success(f"üìä –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {len(models_info)} –º–æ–¥–µ–ª–µ–π")
            return result
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π: {e}")
            return {"error": str(e), "models": []}
    
    def _analyze_gguf_file(self, gguf_path: Path) -> Optional[Dict[str, Any]]:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç GGUF —Ñ–∞–π–ª –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏
        """
        try:
            file_size_mb = gguf_path.stat().st_size / (1024 * 1024)
            file_name = gguf_path.name
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –º–æ–¥–µ–ª–∏ –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
            model_type = "llm"
            lower_name = file_name.lower()
            
            if any(keyword in lower_name for keyword in ['embedding', 'embed', 'encoder']):
                model_type = "embedding"
            
            return {
                "name": file_name,
                "display_name": file_name,  # GGUF —Ñ–∞–π–ª—ã –∏–º–µ—é—Ç –ø–æ–Ω—è—Ç–Ω—ã–µ –∏–º–µ–Ω–∞
                "path": str(gguf_path),
                "size_mb": round(file_size_mb, 2),
                "type": model_type,
                "format": "gguf",
                "is_gguf": True,
                "is_hf": False,
                "files": [{
                    "name": file_name,
                    "size_mb": round(file_size_mb, 2),
                    "relative_path": file_name
                }],
                "is_usable": True
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ GGUF —Ñ–∞–π–ª–∞ {gguf_path}: {e}")
            return None
    
    def _is_valid_model_directory(self, model_dir: Path) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –ø–∞–ø–∫–∞ –†–ï–ê–õ–¨–ù–û–ô —Ä–∞–±–æ—á–µ–π –º–æ–¥–µ–ª—å—é –≤ –°–¢–ê–ù–î–ê–†–¢–ù–û–ú –§–û–†–ú–ê–¢–ï HF
        """
        dir_name = model_dir.name
        
        # –í–°–Å –ü–†–û–°–¢–û: –¢–û–õ–¨–ö–û —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç HF!
        if not dir_name.startswith('models--'):
            return False
        
        # –í–°–Å –ü–†–û–°–¢–û: –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–æ —Ä–µ–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å (–µ—Å—Ç—å –∫–æ–Ω—Ñ–∏–≥)
        has_config = (model_dir / "config.json").exists()
        
        return has_config
    
    def _is_usable_model(self, model_info: Dict) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –º–æ–∂–Ω–æ –ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å
        """
        # –î–ª—è GGUF —Ñ–∞–π–ª–æ–≤ - –≤—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã –µ—Å–ª–∏ —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        if model_info.get('is_gguf', False):
            return Path(model_info['path']).exists()
            
        # –î–ª—è HF –º–æ–¥–µ–ª–µ–π - —Å—Ç–∞—Ä–∞—è –ª–æ–≥–∏–∫–∞
        if model_info.get('size_mb', 0) < 1:  # –°–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∞—è
            return False
            
        if not model_info.get('files'):  # –ù–µ—Ç —Ñ–∞–π–ª–æ–≤
            return False
            
        # –ú–∏–Ω–∏–º—É–º 1 —Ñ–∞–π–ª –º–æ–¥–µ–ª–∏ –∏ 1 –∫–æ–Ω—Ñ–∏–≥
        model_files = [f for f in model_info['files'] if any(ext in f['name'] for ext in ['.bin', '.safetensors', '.pt'])]
        config_files = [f for f in model_info['files'] if 'config.json' in f['name']]
        
        return len(model_files) > 0 and len(config_files) > 0
    
    def analyze_model_directory(self, model_dir: Path) -> Optional[Dict[str, Any]]:
        """
        –î–ï–¢–ê–õ–¨–ù–´–ô –∞–Ω–∞–ª–∏–∑ –ø–∞–ø–∫–∏ —Å –º–æ–¥–µ–ª—å—é (HF —Ñ–æ—Ä–º–∞—Ç)
        """
        try:
            model_name = model_dir.name
            
            # –û–ü–†–ï–î–ï–õ–Ø–ï–ú –¢–ò–ü –ú–û–î–ï–õ–ò –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ
            model_type = self._detect_model_type(model_dir)
            
            info = {
                "name": model_name,
                "display_name": self._get_display_name(model_name),
                "path": str(model_dir),
                "size_mb": self.get_directory_size_mb(model_dir),
                "type": model_type,
                "format": "hf",
                "is_gguf": False,
                "is_hf": True,
                "files": [],
                "is_usable": True
            }
            
            # –ê–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–æ–≤
            for file_path in model_dir.rglob("*"):
                if file_path.is_file():
                    file_info = {
                        "name": file_path.name,
                        "size_mb": round(file_path.stat().st_size / (1024 * 1024), 2),
                        "relative_path": str(file_path.relative_to(model_dir))
                    }
                    info["files"].append(file_info)
            
            # –ü–∞—Ä—Å–∏–º –∫–æ–Ω—Ñ–∏–≥ –¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
            config_info = self.parse_config_file(model_dir)
            info.update(config_info)
            
            return info
            
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –ø–∞–ø–∫–∏ {model_dir}: {e}")
            return None

    def _detect_model_type(self, model_dir: Path) -> str:
        """
        –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –º–æ–¥–µ–ª–∏ –ø–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º Hugging Face
        –°–æ—Ö—Ä–∞–Ω—è–µ–º –í–°–Æ —Å—Ç–∞—Ä—É—é –ª–æ–≥–∏–∫—É –¥–ª—è HF –º–æ–¥–µ–ª–µ–π
        """
        # === 1. –ü–†–û–í–ï–†–ö–ê –ù–ê EMBEDDING –ú–û–î–ï–õ–ò ===
        if (model_dir / "config_sentence_transformers.json").exists():
            return "embedding"
        
        if (model_dir / "modules.json").exists():
            return "embedding"
            
        # === 2. –ü–†–û–í–ï–†–ö–ê –ü–û –û–°–ù–û–í–ù–û–ú–£ CONFIG.JSON ===
        try:
            config_path = model_dir / "config.json"
            if config_path.exists():
                with open(config_path, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                
                embedding_architectures = [
                    "SentenceTransformer", "Transformer", "EmbeddingModel",
                    "XLMRobertaModel", "MPNetModel", "DistilBertModel"
                ]
                
                llm_architectures = [
                    "Qwen2ForCausalLM", "LlamaForCausalLM", "GPT2LMHeadModel",
                    "MistralForCausalLM", "PhiForCausalLM", "BloomForCausalLM"
                ]
                
                architectures = config.get("architectures", [])
                model_type = config.get("model_type", "")
                
                if any(arch in str(architectures) for arch in embedding_architectures):
                    return "embedding"
                    
                if any(arch in str(architectures) for arch in llm_architectures):
                    return "llm"
                
                if any(tipo in model_type for tipo in ["sentence_transformers", "embedding"]):
                    return "embedding"
                elif any(tipo in model_type for tipo in ["text-generation", "causal-lm"]):
                    return "llm"
                    
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è config.json –¥–ª—è {model_dir}: {e}")
        
        # === 3. –ü–†–û–í–ï–†–ö–ê –ü–û –§–ê–ô–õ–ê–ú –¢–û–ö–ï–ù–ê–ô–ó–ï–†–ê ===
        tokenizer_files = list(model_dir.glob("tokenizer*")) + list(model_dir.glob("*vocab*"))
        if tokenizer_files:
            return "llm"
        
        # === 4. –†–ï–ó–ï–†–í–ù–´–ô –í–ê–†–ò–ê–ù–¢: –ü–û –ò–ú–ï–ù–ò –ü–ê–ü–ö–ò ===
        dir_name = model_dir.name.lower()
        
        embedding_keywords = ['e5', 'embedding', 'sentence', 'transformers', 'mpnet', 'minilm']
        llm_keywords = ['qwen', 'chat', 'instruct', 'gpt', 'llama', 'mistral', 'phi']
        
        if any(keyword in dir_name for keyword in embedding_keywords):
            return "embedding"
        elif any(keyword in dir_name for keyword in llm_keywords):
            return "llm"
        
        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø –º–æ–¥–µ–ª–∏: {model_dir.name}")
        return "unknown"
    
    def _get_display_name(self, model_dir_name: str) -> str:
        """
        –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –∏–º—è –ø–∞–ø–∫–∏ –≤ —á–∏—Ç–∞–µ–º–æ–µ –∏–º—è –º–æ–¥–µ–ª–∏
        """
        if 'models--' in model_dir_name:
            return model_dir_name.replace('models--', '').replace('--', '/')
        else:
            return model_dir_name
    
    def get_directory_size_mb(self, directory: Path) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –ø–∞–ø–∫–∏ –≤ MB"""
        total_size = 0
        for file_path in directory.rglob("*"):
            if file_path.is_file():
                total_size += file_path.stat().st_size
        return round(total_size / (1024 * 1024), 2)
    
    def parse_config_file(self, model_dir: Path) -> Dict[str, Any]:
        """–ü–∞—Ä—Å–∏–Ω–≥ config.json –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–∏"""
        try:
            config_path = next(model_dir.rglob("config.json"))
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            
            info = {}
            
            if "architectures" in config:
                info["architecture"] = config["architectures"][0] if config["architectures"] else "unknown"
            
            if "model_type" in config:
                info["model_type"] = config["model_type"]
            
            if "vocab_size" in config:
                info["vocab_size"] = config["vocab_size"]
                
            if "hidden_size" in config:
                info["hidden_size"] = config["hidden_size"]
            
            return info
            
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ config.json: {e}")
            return {}

    def analyze_model_cache(self) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª–∏–∑ –∫–µ—à–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –±–∏—Ç—ã—Ö –∏ –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –º–æ–¥–µ–ª–µ–π
        """
        try:
            cache_info = self.scan_models_cache()
            broken_models = []
            
            for model in cache_info.get("models", []):
                if not model.get("is_usable", True):
                    broken_models.append(model["name"])
            
            return {
                "total_models": cache_info["total_models"],
                "broken_models": broken_models,
                "usable_models": [m["name"] for m in cache_info["models"] if m.get("is_usable", True)]
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–µ—à–∞: {e}")
            return {"error": str(e)}

    def _get_local_model_path(self, model_name: str):
        """–ü–æ–ª—É—á–∞–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–π –ø—É—Ç—å –∫ –º–æ–¥–µ–ª–∏"""
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º GGUF —Ñ–∞–π–ª—ã
        gguf_path = self.models_cache / model_name
        if gguf_path.exists() and gguf_path.is_file() and gguf_path.suffix.lower() == '.gguf':
            return gguf_path
        
        # –ó–∞—Ç–µ–º –ø—Ä–æ–≤–µ—Ä—è–µ–º HF –ø–∞–ø–∫–∏
        –≤–æ–∑–º–æ–∂–Ω—ã–µ_–ø—É—Ç–∏ = []
        
        if 'models--' in model_name:
            –≤–æ–∑–º–æ–∂–Ω—ã–µ_–ø—É—Ç–∏.append(self.models_cache / model_name)
        else:
            cache_name = f"models--{model_name.replace('/', '--')}"
            –≤–æ–∑–º–æ–∂–Ω—ã–µ_–ø—É—Ç–∏.append(self.models_cache / cache_name)
            –≤–æ–∑–º–æ–∂–Ω—ã–µ_–ø—É—Ç–∏.append(self.models_cache / model_name)
        
        for –ø—É—Ç—å in –≤–æ–∑–º–æ–∂–Ω—ã–µ_–ø—É—Ç–∏:
            if –ø—É—Ç—å.exists():
                return –ø—É—Ç—å
        
        return None

    def delete_model(self, model_name: str) -> bool:
        """–£–¥–∞–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏–∑ –∫–µ—à–∞"""
        try:
            local_path = self._get_local_model_path(model_name)
            if local_path and local_path.exists():
                import shutil
                if local_path.is_file():
                    local_path.unlink()  # –£–¥–∞–ª—è–µ–º GGUF —Ñ–∞–π–ª
                else:
                    shutil.rmtree(local_path)  # –£–¥–∞–ª—è–µ–º HF –ø–∞–ø–∫—É
                logger.info(f"–ú–æ–¥–µ–ª—å {model_name} —É–¥–∞–ª–µ–Ω–∞ –∏–∑ –∫–µ—à–∞")
                return True
            logger.warning(f"–ú–æ–¥–µ–ª—å {model_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è: {local_path}")
            return False
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ {model_name}: {e}")
            return False

    def download_model(self, model_id: str) -> bool:
        """–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏–∑ HuggingFace Hub"""
        try:
            logger.info(f"–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏: {model_id}")
            
            # –°–æ–∑–¥–∞–µ–º –∏–º—è –ø–∞–ø–∫–∏ –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
            cache_name = f"models--{model_id.replace('/', '--')}"
            local_dir = self.models_cache / cache_name
            
            snapshot_download(
                repo_id=model_id,
                local_dir=local_dir,
                local_dir_use_symlinks=False,
                resume_download=True
            )
            
            logger.info(f"–ú–æ–¥–µ–ª—å {model_id} —É—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω–∞ –≤ {local_dir}")
            return True
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ {model_id}: {e}")
            return False

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–µ—Ä–≤–∏—Å–∞
model_discovery = ModelDiscoveryService()
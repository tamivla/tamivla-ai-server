# src/services/model_manager.py
"""
–ú–µ–Ω–µ–¥–∂–µ—Ä –º–æ–¥–µ–ª–µ–π –¥–ª—è Tamivla AI Server
–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∑–∞–≥—Ä—É–∑–∫–æ–π –∏ –≤—ã–≥—Ä—É–∑–∫–æ–π AI-–º–æ–¥–µ–ª–µ–π –¢–û–õ–¨–ö–û –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∫–µ—à–∞
"""

import os
import gc
import torch
from typing import Dict, Any, Optional
from loguru import logger
from pathlib import Path

class ModelManager:
    """–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∂–∏–∑–Ω–µ–Ω–Ω—ã–º —Ü–∏–∫–ª–æ–º AI-–º–æ–¥–µ–ª–µ–π"""
    
    def __init__(self):
        self.loaded_models: Dict[str, Any] = {}
        self.models_cache = Path(os.environ.get('HF_HOME', 'storage/models'))
        
    def _normalize_model_name(self, model_name: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –∏–º—è –º–æ–¥–µ–ª–∏ –∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º—É —Ñ–æ—Ä–º–∞—Ç—É HF –∫–µ—à–∞"""
        if model_name.startswith('models--'):
            return model_name  # –£–∂–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–æ
        elif '/' in model_name:
            return f"models--{model_name.replace('/', '--')}"  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º
        else:
            return model_name  # –û—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
    
    def preload_essential_models(self):
        """–ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ –æ—Å–Ω–æ–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ —Å–µ—Ä–≤–µ—Ä–∞"""
        essential_models = {
            "intfloat/multilingual-e5-large-instruct": "embedding"
        }
        
        for model_name, model_type in essential_models.items():
            if self._get_local_model_path(model_name):
                if self.load_model(model_name, model_type):
                    logger.info(f"‚úÖ –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model_name}")
                else:
                    logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∏—Ç—å: {model_name}")
            else:
                logger.warning(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å {model_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –¥–ª—è –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∏")

    def is_model_loaded(self, model_name: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –ª–∏ –º–æ–¥–µ–ª—å (—Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π –∏–º–µ–Ω–∏)"""
        normalized_name = self._normalize_model_name(model_name)
        return normalized_name in self.loaded_models
        
    def load_model(self, model_name: str, model_type: str, **kwargs) -> bool:
        try:
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∏–º—è –î–û –ø—Ä–æ–≤–µ—Ä–∫–∏
            normalized_name = self._normalize_model_name(model_name)
            if normalized_name in self.loaded_models:
                logger.info(f"–ú–æ–¥–µ–ª—å {normalized_name} —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
                return True
                
            logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ {model_type} –º–æ–¥–µ–ª–∏: {normalized_name}")
            
            # –ñ–ï–°–¢–ö–ê–Ø –ü–†–û–í–ï–†–ö–ê: –º–æ–¥–µ–ª—å –î–û–õ–ñ–ù–ê —Å—É—â–µ—Å—Ç–≤–æ–≤–∞—Ç—å –ª–æ–∫–∞–ª—å–Ω–æ
            local_path = self._get_local_model_path(model_name)  # ‚Üê –ü–µ—Ä–µ–¥–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –∏–º—è
            if not local_path:
                logger.error(f"üö´ –ó–ê–ü–†–ï–©–ï–ù–û: –ú–æ–¥–µ–ª—å {model_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –ª–æ–∫–∞–ª—å–Ω–æ–º –∫–µ—à–µ")
                return False
            
            # –ó–ê–ü–†–ï–¢ –Ω–∞ –∞–≤—Ç–æ–∑–∞–≥—Ä—É–∑–∫—É —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
            os.environ['TRANSFORMERS_OFFLINE'] = '1'
            os.environ['HF_DATASETS_OFFLINE'] = '1'
            
            # –ó–ê–ì–†–£–ñ–ê–ï–ú –ò–°–ö–õ–Æ–ß–ò–¢–ï–õ–¨–ù–û –ò–ó –õ–û–ö–ê–õ–¨–ù–û–ì–û –ü–£–¢–Ø!
            if model_type == 'embedding':
                from sentence_transformers import SentenceTransformer
                try:
                    # –ó–ê–ì–†–£–ñ–ê–ï–ú –ü–†–Ø–ú–û –ò–ó –ü–£–¢–ò!
                    model = SentenceTransformer(
                        str(local_path),  # ‚Üê –í–û–¢ –û–ù–û! –õ–û–ö–ê–õ–¨–ù–´–ô –ü–£–¢–¨!
                        device='cuda' if torch.cuda.is_available() else 'cpu'
                    )
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ SentenceTransformer: {e}")
                    return False
                    
            elif model_type == 'llm':
                from transformers import pipeline
                model = pipeline(
                    "text-generation",
                    model=str(local_path),  # ‚Üê –í–û–¢ –û–ù–û! –õ–û–ö–ê–õ–¨–ù–´–ô –ü–£–¢–¨!
                    tokenizer=str(local_path),
                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                    device_map="auto" if torch.cuda.is_available() else None
                )
            else:
                raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø –º–æ–¥–µ–ª–∏: {model_type}")
            
            self.loaded_models[normalized_name] = {
                'type': model_type,
                'status': 'loaded',
                'model': model,
                'device': 'cuda' if torch.cuda.is_available() else 'cpu',
                'local_path': str(local_path)
            }
            
            logger.success(f"–ú–æ–¥–µ–ª—å {normalized_name} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∫–µ—à–∞")
            return True
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ {model_name}: {e}")
            return False
    
    def _get_local_model_path(self, model_name: str) -> Optional[Path]:
        """–ü–æ–ª—É—á–∞–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–π –ø—É—Ç—å –∫ –º–æ–¥–µ–ª–∏ –ø–æ HF —Å—Ç–∞–Ω–¥–∞—Ä—Ç—É"""
        
        # üî¥ –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –í—Å–µ–≥–¥–∞ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∏–º—è –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ –∫–µ—à–µ
        normalized_name = self._normalize_model_name(model_name)
        
        –ø—É—Ç—å = self.models_cache / normalized_name
        if –ø—É—Ç—å.exists():
            logger.info(f"–ù–∞–π–¥–µ–Ω –ø—É—Ç—å: {–ø—É—Ç—å}")
            return –ø—É—Ç—å
        
        logger.error(f"–ú–æ–¥–µ–ª—å {model_name} (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–æ: {normalized_name}) –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –∫–µ—à–µ")
        return None
    
    def unload_model(self, model_name: str) -> bool:
        """–í—ã–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ –ø–∞–º—è—Ç–∏"""
        try:
            normalized_name = self._normalize_model_name(model_name)
            if normalized_name not in self.loaded_models:
                logger.warning(f"–ú–æ–¥–µ–ª—å {normalized_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö")
                return False
                
            logger.info(f"–í—ã–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {normalized_name}")
            
            # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º —Ä–µ—Å—É—Ä—Å—ã
            model_info = self.loaded_models.pop(normalized_name)
            if model_info.get('model'):
                del model_info['model']
                
            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Å–±–æ—Ä–∫–∞ –º—É—Å–æ—Ä–∞
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            logger.success(f"–ú–æ–¥–µ–ª—å {normalized_name} —É—Å–ø–µ—à–Ω–æ –≤—ã–≥—Ä—É–∂–µ–Ω–∞")
            return True
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤—ã–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ {model_name}: {e}")
            return False
    
    def get_model(self, model_name: str) -> Optional[Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        normalized_name = self._normalize_model_name(model_name)
        if normalized_name in self.loaded_models:
            return self.loaded_models[normalized_name]['model']
        return None
    
    def get_model_info(self, model_name: str) -> Optional[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª–∏"""
        normalized_name = self._normalize_model_name(model_name)
        return self.loaded_models.get(normalized_name)
    
    def list_loaded_models(self) -> Dict[str, str]:
        """–°–ø–∏—Å–æ–∫ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        return {name: info['type'] for name, info in self.loaded_models.items()}
    
    def get_model_stats(self) -> Dict[str, Any]:
        """–ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –º–æ–¥–µ–ª—è–º"""
        return {
            'total_loaded': len(self.loaded_models),
            'embedding_models': sum(1 for info in self.loaded_models.values() 
                                  if info['type'] == 'embedding'),
            'llm_models': sum(1 for info in self.loaded_models.values() 
                            if info['type'] == 'llm'),
            'models': self.list_loaded_models()
        }

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –º–æ–¥–µ–ª–µ–π
model_manager = ModelManager()
# src/services/model_manager.py
"""
–ú–µ–Ω–µ–¥–∂–µ—Ä –º–æ–¥–µ–ª–µ–π –¥–ª—è Tamivla AI Server
–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∑–∞–≥—Ä—É–∑–∫–æ–π –∏ –≤—ã–≥—Ä—É–∑–∫–æ–π AI-–º–æ–¥–µ–ª–µ–π –¢–û–õ–¨–ö–û –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∫–µ—à–∞
"""

import os
import gc
import torch
from typing import Dict, Any, Optional
from loguru import logger
from pathlib import Path

class ModelManager:
    """–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∂–∏–∑–Ω–µ–Ω–Ω—ã–º —Ü–∏–∫–ª–æ–º AI-–º–æ–¥–µ–ª–µ–π"""
    
    def __init__(self):
        self.loaded_models: Dict[str, Any] = {}
        self.models_cache = Path(os.environ.get('HF_HOME', 'storage/models'))
        
    def load_model(self, model_name: str, model_type: str, **kwargs) -> bool:
        try:
            if model_name in self.loaded_models:
                logger.info(f"–ú–æ–¥–µ–ª—å {model_name} —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
                return True
                
            logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ {model_type} –º–æ–¥–µ–ª–∏: {model_name}")
            
            # –ñ–ï–°–¢–ö–ê–Ø –ü–†–û–í–ï–†–ö–ê: –º–æ–¥–µ–ª—å –î–û–õ–ñ–ù–ê —Å—É—â–µ—Å—Ç–≤–æ–≤–∞—Ç—å –ª–æ–∫–∞–ª—å–Ω–æ
            local_path = self._get_local_model_path(model_name)
            if not local_path:
                logger.error(f"üö´ –ó–ê–ü–†–ï–©–ï–ù–û: –ú–æ–¥–µ–ª—å {model_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –ª–æ–∫–∞–ª—å–Ω–æ–º –∫–µ—à–µ")
                return False
            
            # –ó–ê–ü–†–ï–¢ –Ω–∞ –∞–≤—Ç–æ–∑–∞–≥—Ä—É–∑–∫—É —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
            os.environ['TRANSFORMERS_OFFLINE'] = '1'
            os.environ['HF_DATASETS_OFFLINE'] = '1'
            
            # –ó–ê–ì–†–£–ñ–ê–ï–ú –ò–°–ö–õ–Æ–ß–ò–¢–ï–õ–¨–ù–û –ò–ó –õ–û–ö–ê–õ–¨–ù–û–ì–û –ü–£–¢–ò!
            if model_type == 'embedding':
                from sentence_transformers import SentenceTransformer
                try:
                    # –ó–ê–ì–†–£–ñ–ê–ï–ú –ü–†–Ø–ú–û –ò–ó –ü–£–¢–ò!
                    model = SentenceTransformer(
                        str(local_path),  # ‚Üê –í–û–¢ –û–ù–û! –õ–û–ö–ê–õ–¨–ù–´–ô –ü–£–¢–¨!
                        device='cuda' if torch.cuda.is_available() else 'cpu'
                    )
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ SentenceTransformer: {e}")
                    return False
                    
            elif model_type == 'llm':
                from transformers import pipeline
                model = pipeline(
                    "text-generation",
                    model=str(local_path),  # ‚Üê –í–û–¢ –û–ù–û! –õ–û–ö–ê–õ–¨–ù–´–ô –ü–£–¢–¨!
                    tokenizer=str(local_path),
                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                    device_map="auto" if torch.cuda.is_available() else None
                )
            else:
                raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø –º–æ–¥–µ–ª–∏: {model_type}")
            
            self.loaded_models[model_name] = {
                'type': model_type,
                'status': 'loaded',
                'model': model,
                'device': 'cuda' if torch.cuda.is_available() else 'cpu',
                'local_path': str(local_path)
            }
            
            logger.success(f"–ú–æ–¥–µ–ª—å {model_name} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –∫–µ—à–∞")
            return True
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ {model_name}: {e}")
            return False
    
    def _convert_cache_name_to_model_name(self, cache_name: str) -> str:
        """
        –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –∏–º—è –∏–∑ –∫–µ—à–∞ –≤ –∏–º—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –ò–°–ö–õ–Æ–ß–ò–¢–ï–õ–¨–ù–û –ø–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—É HF
        """
        # –¢–û–õ–¨–ö–û —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç HF!
        if cache_name.startswith('models--'):
            return cache_name.replace('models--', '').replace('--', '/')
        
        # –ï—Å–ª–∏ –Ω–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç - –û–®–ò–ë–ö–ê!
        raise ValueError(f"–ú–æ–¥–µ–ª—å {cache_name} –≤ –Ω–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ HF")
    
    def _get_local_model_path(self, model_name: str) -> Optional[Path]:
        """–ü–æ–ª—É—á–∞–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–π –ø—É—Ç—å –∫ –º–æ–¥–µ–ª–∏ –ø–æ –°–¢–ê–ù–î–ê–†–¢–ù–û–ú–£ –§–û–†–ú–ê–¢–£ HF"""
        
        # –¢–û–õ–¨–ö–û —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç: models--author--model-name
        if not model_name.startswith('models--'):
            # –ü—Ä–æ–±—É–µ–º –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –∫—Ä–∞—Å–∏–≤–æ–µ –∏–º—è –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
            if '/' in model_name:
                model_name = f"models--{model_name.replace('/', '--')}"
            else:
                logger.error(f"–ú–æ–¥–µ–ª—å {model_name} –≤ –Ω–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ")
                return None
        
        –ø—É—Ç—å = self.models_cache / model_name
        if –ø—É—Ç—å.exists():
            logger.info(f"–ù–∞–π–¥–µ–Ω —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø—É—Ç—å: {–ø—É—Ç—å}")
            return –ø—É—Ç—å
        
        logger.error(f"–ú–æ–¥–µ–ª—å {model_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ")
        return None
    
    def unload_model(self, model_name: str) -> bool:
        """–í—ã–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ –ø–∞–º—è—Ç–∏"""
        try:
            if model_name not in self.loaded_models:
                logger.warning(f"–ú–æ–¥–µ–ª—å {model_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö")
                return False
                
            logger.info(f"–í—ã–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {model_name}")
            
            # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º —Ä–µ—Å—É—Ä—Å—ã
            model_info = self.loaded_models.pop(model_name)
            if model_info.get('model'):
                del model_info['model']
                
            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Å–±–æ—Ä–∫–∞ –º—É—Å–æ—Ä–∞
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            logger.success(f"–ú–æ–¥–µ–ª—å {model_name} —É—Å–ø–µ—à–Ω–æ –≤—ã–≥—Ä—É–∂–µ–Ω–∞")
            return True
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤—ã–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ {model_name}: {e}")
            return False
    
    def get_model(self, model_name: str) -> Optional[Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        if model_name in self.loaded_models:
            return self.loaded_models[model_name]['model']
        return None
    
    def get_model_info(self, model_name: str) -> Optional[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª–∏"""
        return self.loaded_models.get(model_name)
    
    def list_loaded_models(self) -> Dict[str, str]:
        """–°–ø–∏—Å–æ–∫ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        return {name: info['type'] for name, info in self.loaded_models.items()}
    
    def get_model_stats(self) -> Dict[str, Any]:
        """–ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –º–æ–¥–µ–ª—è–º"""
        return {
            'total_loaded': len(self.loaded_models),
            'embedding_models': sum(1 for info in self.loaded_models.values() 
                                  if info['type'] == 'embedding'),
            'llm_models': sum(1 for info in self.loaded_models.values() 
                            if info['type'] == 'llm'),
            'models': self.list_loaded_models()
        }

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –º–æ–¥–µ–ª–µ–π
model_manager = ModelManager()
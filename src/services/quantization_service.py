"""
–°–µ—Ä–≤–∏—Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π
–£–º–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ–π VRAM –∏ –∞–≤—Ç–æ-–∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ–º
"""

import os
import torch
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from loguru import logger

class QuantizationService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π –ø–æ–¥ –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã"""
    
    def __init__(self):
        self.quantized_models_cache = Path(os.environ.get('HF_HOME', 'storage/models')) / "quantized"
        self.quantized_models_cache.mkdir(exist_ok=True)
        
    def get_gpu_memory_info(self) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ GPU –ø–∞–º—è—Ç–∏
        """
        try:
            if not torch.cuda.is_available():
                return {
                    "available": False,
                    "error": "CUDA not available", 
                    "gpus": {}
                }
            
            gpu_info = {}
            for i in range(torch.cuda.device_count()):
                props = torch.cuda.get_device_properties(i)
                allocated = torch.cuda.memory_allocated(i) / (1024**3)  # GB
                reserved = torch.cuda.memory_reserved(i) / (1024**3)    # GB
                total = props.total_memory / (1024**3)                  # GB
                free = total - allocated
                
                gpu_info[f"cuda:{i}"] = {
                    "name": props.name,
                    "total_gb": round(total, 2),
                    "allocated_gb": round(allocated, 2),
                    "reserved_gb": round(reserved, 2),
                    "free_gb": round(free, 2),
                    "free_percent": round((free / total) * 100, 1),
                    "compute_capability": f"{props.major}.{props.minor}",
                    "multi_processor_count": props.multi_processor_count
                }
            
            result = {
                "available": True,
                "gpus": gpu_info
            }
            
            if gpu_info:
                result["primary_gpu"] = list(gpu_info.keys())[0]
            
            return result
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ GPU: {e}")
            return {
                "available": False,
                "error": str(e),
                "gpus": {}
            }
    
    def calculate_optimal_quantization(self, model_size_gb: float, target_device: str = "cuda:0") -> Dict[str, Any]:
        """
        –†–∞—Å—á–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —É—Ä–æ–≤–Ω—è –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è –¥–ª—è –º–æ–¥–µ–ª–∏
        
        Args:
            model_size_gb: –†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏ –≤ GB
            target_device: –¶–µ–ª–µ–≤–æ–µ GPU —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏ –ø–æ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—é
        """
        gpu_info = self.get_gpu_memory_info()
        
        if not gpu_info["available"]:
            return {
                "recommended": "cpu",
                "reason": "GPU not available",
                "can_load": False,
                "estimated_vram_usage_gb": model_size_gb
            }
        
        target_gpu = gpu_info["gpus"].get(target_device)
        if not target_gpu:
            return {
                "recommended": "cpu", 
                "reason": "Target GPU not found",
                "can_load": False,
                "estimated_vram_usage_gb": model_size_gb
            }
        
        free_vram = target_gpu["free_gb"]
        total_vram = target_gpu["total_gb"]
        
        # –†–∞—Å—á–µ—Ç –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è
        quantization_levels = {
            "fp32": {"bits": 32, "reduction": 1.0, "quality": "original"},
            "fp16": {"bits": 16, "reduction": 0.5, "quality": "excellent"}, 
            "bf16": {"bits": 16, "reduction": 0.5, "quality": "excellent"},
            "8bit": {"bits": 8, "reduction": 0.25, "quality": "very good"},
            "4bit": {"bits": 4, "reduction": 0.125, "quality": "good"},
            "q4": {"bits": 4, "reduction": 0.125, "quality": "good"}
        }
        
        recommendations = []
        
        for level_name, level_info in quantization_levels.items():
            estimated_size = model_size_gb * level_info["reduction"]
            safety_margin = 1.2  # 20% –∑–∞–ø–∞—Å –¥–ª—è overhead
            required_vram = estimated_size * safety_margin
            
            can_fit = required_vram <= free_vram
            vram_usage_percent = (required_vram / total_vram) * 100
            
            recommendations.append({
                "level": level_name,
                "bits": level_info["bits"],
                "estimated_size_gb": round(estimated_size, 2),
                "required_vram_gb": round(required_vram, 2),
                "can_fit": can_fit,
                "vram_usage_percent": round(vram_usage_percent, 1),
                "quality": level_info["quality"],
                "recommended": can_fit and level_info["bits"] <= 8  # –ü—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º 8-bit –∏–ª–∏ –º–µ–Ω—å—à–µ
            })
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É (—Å–Ω–∞—á–∞–ª–∞ —Ç–µ —á—Ç–æ –≤–ª–µ–∑–∞—é—Ç, –ø–æ—Ç–æ–º –ø–æ –∫–∞—á–µ—Å—Ç–≤—É)
        recommendations.sort(key=lambda x: (not x["can_fit"], x["bits"]))
        
        # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à—É—é —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é
        best_recommendation = None
        for rec in recommendations:
            if rec["can_fit"]:
                best_recommendation = rec
                break
        
        if not best_recommendation:
            # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –≤–ª–µ–∑–∞–µ—Ç, –ø—Ä–µ–¥–ª–∞–≥–∞–µ–º —Å–∞–º–æ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ
            best_recommendation = recommendations[-1]
            best_recommendation["forced"] = True
            best_recommendation["warning"] = f"–ú–æ–¥–µ–ª—å –Ω–µ –≤–ª–µ–∑–∞–µ—Ç –¥–∞–∂–µ —Å –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ–º. –¢—Ä–µ–±—É–µ—Ç—Å—è {best_recommendation['required_vram_gb']}GB, –¥–æ—Å—Ç—É–ø–Ω–æ {free_vram}GB"
        
        return {
            "model_size_gb": model_size_gb,
            "target_gpu": target_gpu,
            "free_vram_gb": free_vram,
            "total_vram_gb": total_vram,
            "recommendations": recommendations,
            "best_recommendation": best_recommendation,
            "can_load": best_recommendation["can_fit"] if not best_recommendation.get("forced") else False
        }
    
    def get_model_size_estimation(self, model_name: str) -> float:
        """
        –û—Ü–µ–Ω–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏ –ø–æ –µ—ë –∏–º–µ–Ω–∏ –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        """
        # –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–µ –æ—Ü–µ–Ω–∫–∏ —Ä–∞–∑–º–µ—Ä–æ–≤ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        model_size_estimations = {
            "Qwen2.5-7B": 14.5,
            "Qwen2.5-14B": 28.0,
            "Qwen2-7B": 14.0,
            "Qwen2-1.5B": 3.0,
            "Llama-3-8B": 16.0,
            "Llama-3-70B": 140.0,
            "mistral-7b": 14.0,
            "mixtral-8x7b": 45.0,
            "all-MiniLM-L6-v2": 0.09,
            "all-mpnet-base-v2": 0.42,
            "paraphrase-multilingual-mpnet-base-v2": 2.1,
            "multilingual-e5-large": 2.2
        }
        
        # –ò—â–µ–º –ø–æ–¥—Ö–æ–¥—è—â—É—é –æ—Ü–µ–Ω–∫—É
        for pattern, size in model_size_estimations.items():
            if pattern.lower() in model_name.lower():
                return size
        
        # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ —Å–ø–∏—Å–∫–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º —ç–≤—Ä–∏—Å—Ç–∏–∫—É
        if "7b" in model_name.lower() or "7B" in model_name:
            return 14.0
        elif "13b" in model_name.lower() or "13B" in model_name:
            return 26.0
        elif "70b" in model_name.lower() or "70B" in model_name:
            return 140.0
        else:
            return 2.0  # –î–µ—Ñ–æ–ª—Ç–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –¥–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
    
    def generate_quantization_suggestions(self, model_name: str) -> Dict[str, Any]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –ø–æ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—é –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏
        """
        model_size = self.get_model_size_estimation(model_name)
        quantization_analysis = self.calculate_optimal_quantization(model_size)
        
        return {
            "model_name": model_name,
            "estimated_size_gb": model_size,
            "quantization_analysis": quantization_analysis,
            "suggestions": self._generate_human_readable_suggestions(quantization_analysis)
        }
    
    def _generate_human_readable_suggestions(self, analysis: Dict[str, Any]) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ–ª–æ–≤–µ–∫–æ-—á–∏—Ç–∞–µ–º—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π"""
        suggestions = []
        
        model_size = analysis["model_size_gb"]
        free_vram = analysis["free_vram_gb"]
        best_rec = analysis["best_recommendation"]
        
        suggestions.append(f"üíæ –†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏: {model_size} GB")
        suggestions.append(f"üéÆ –î–æ—Å—Ç—É–ø–Ω–æ VRAM: {free_vram} GB")
        
        if analysis["can_load"]:
            suggestions.append(f"‚úÖ –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è: {best_rec['level']} ({best_rec['bits']}-bit)")
            suggestions.append(f"üìä –ö–∞—á–µ—Å—Ç–≤–æ: {best_rec['quality']}")
            suggestions.append(f"üîÆ –ó–∞–π–º–µ—Ç VRAM: ~{best_rec['estimated_size_gb']} GB")
        else:
            suggestions.append(f"‚ö†Ô∏è  –ú–æ–¥–µ–ª—å –Ω–µ –≤–ª–µ–∑–∞–µ—Ç –≤ –¥–æ—Å—Ç—É–ø–Ω—É—é –ø–∞–º—è—Ç—å")
            suggestions.append(f"üí° –ú–æ–∂–Ω–æ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å: {best_rec['level']} ({best_rec['bits']}-bit)")
            suggestions.append(f"üîÆ –ü–æ—Ç—Ä–µ–±—É–µ—Ç—Å—è: ~{best_rec['required_vram_gb']} GB")
            suggestions.append("üö® –í–æ–∑–º–æ–∂–Ω—ã –ø—Ä–æ–±–ª–µ–º—ã —Å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é")
        
        # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã
        alt_options = [rec for rec in analysis["recommendations"] if rec["can_fit"] and rec != best_rec]
        if alt_options:
            suggestions.append("\nüîß –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã:")
            for opt in alt_options[:2]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ 2 –ª—É—á—à–∏—Ö –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã
                suggestions.append(f"   ‚Ä¢ {opt['level']} ({opt['bits']}-bit) - {opt['estimated_size_gb']} GB")
        
        return suggestions
    
    def get_quantized_model_path(self, model_name: str, quantization_level: str) -> Path:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—É—Ç–∏ –¥–ª—è –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–∏ –º–æ–¥–µ–ª–∏
        """
        safe_name = model_name.replace('/', '--')
        return self.quantized_models_cache / f"{safe_name}--{quantization_level}"
    
    def is_model_quantized(self, model_name: str, quantization_level: str) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —É–∂–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –º–æ–¥–µ–ª–∏
        """
        quantized_path = self.get_quantized_model_path(model_name, quantization_level)
        return quantized_path.exists()

    def quantize_model(self, model_name: str, quantization_level: str) -> Dict[str, Any]:
        """
        –ö–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º transformers (–æ–±—Ö–æ–¥ –ø—Ä–æ–±–ª–µ–º—ã bitsandbytes –≤ Windows)
        """
        try:
            logger.info(f"üîÑ –ó–∞–ø—É—Å–∫ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è {model_name} –≤ {quantization_level}")
            
            from transformers import AutoModelForCausalLM, BitsAndBytesConfig
            import torch
            
            logger.info(f"üì• –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ")
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è
            if quantization_level == '4bit':
                bnb_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_compute_dtype=torch.float16
                )
            elif quantization_level == '8bit':
                bnb_config = BitsAndBytesConfig(load_in_8bit=True)
            elif quantization_level == 'fp16':
                bnb_config = None  # –ë—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–±—ã—á–Ω—É—é –∑–∞–≥—Ä—É–∑–∫—É —Å fp16
            else:
                return {'error': f'Unsupported quantization level: {quantization_level}'}
            
            logger.info(f"üîß –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è —Å–æ–∑–¥–∞–Ω–∞: {quantization_level}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —Å –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ–º
            logger.info(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {model_name}...")
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                quantization_config=bnb_config,
                device_map="auto",
                trust_remote_code=True,
                torch_dtype=torch.float16 if quantization_level == 'fp16' else None
            )
            
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —Å –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ–º")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å
            quantized_path = self.get_quantized_model_path(model_name, quantization_level)
            logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –≤ {quantized_path}...")
            model.save_pretrained(quantized_path)
            
            logger.info(f"üéâ –ö–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ")
            
            return {
                'success': True,
                'quantized_path': str(quantized_path),
                'model_name': model_name,
                'quantization_level': quantization_level,
                'message': f'–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∞ –≤ {quantization_level}'
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è: {e}")
            import traceback
            logger.error(f"‚ùå Traceback: {traceback.format_exc()}")
            return {
                'success': False,
                'error': str(e),
                'model_name': model_name,
                'quantization_level': quantization_level
            }

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–µ—Ä–≤–∏—Å–∞
quantization_service = QuantizationService()